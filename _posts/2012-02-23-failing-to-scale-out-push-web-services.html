---
layout: post
title: Failing to Scale Out Push Web Services
date: 2012-02-23 05:28:11.000000000 +00:00
categories:
- Future
tags: []
status: publish
type: post
published: true
meta:
  _wpcom_is_markdown: '1'
  tmac_last_id: '531409127738994688'
  _edit_last: '1'
  _jetpack_related_posts_cache: a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1415507585;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:19;}i:1;a:1:{s:2:"id";i:315;}i:2;a:1:{s:2:"id";i:66;}}}}
author:
  login: admin
  email: brunofr@olympum.com
  display_name: Bruno Fernandez-Ruiz
  first_name: Bruno
  last_name: Fernandez-Ruiz
---
<p><strong>Problem</strong>: <em>on the web, enable a large number of message producers send a<br />
very large number of messages to a much larger number of message consumers</em>.<br />
Example: allow 100,000 publishers send a total of 1 million messages per<br />
second to 100 million concurrently connected consumers.</p>
<p>We are dealing with the problem of <em>connection channels</em>, an abstraction that<br />
allows a producer distribute the message to many connected consumers. Our<br />
challenge is to design a distributed channel delivery mechanism that can scale<br />
out to millions of connected consumers. Throughout, our assumption is that<br />
this is a stateless delivery system, i.e. messages are either delivered or<br />
dropped and no persistence guarantees exists; if a consumer is not connected,<br />
it will miss the message.</p>
<p>The na√Øve approach is to perform <strong>consistent hashing by channel</strong>. In this<br />
model, each channel and all its consumers are in the same server. Since the<br />
channel identifier is part of the URI, the load balancer can effectively<br />
perform this operation, and we can add servers as required without requiring<br />
re-balancing. When we have many channels per server, the distribution is<br />
eventually uniform. Problems arise however as some channels have an order of<br />
magnitude more consumers than other channels. There is also a problem if a<br />
channel has more consumers than a server can sustain.</p>
<p>To solve the limitations of hashing by channel, we can instead perform<br />
<strong>consistent hashing by channel and connection (consumer)</strong>. In this model,<br />
each consumer is consistently assigned to a pool of servers and we can add<br />
servers without having to re-balance consumers among servers. The channel<br />
stores a list of all the consumer identifiers and channels are consistently<br />
hashed across servers. To deliver a message, the load balancer will find the<br />
server holding the channel, and dispatch the request. The channel will lookup<br />
the list of consumer identifiers and again apply the consistent hashing<br />
algorithm to reach all the consumers.</p>
<p>Although the hashing by channel and connection is conceptually simple, it<br />
presents significant operability challenges. First, the loss of the server<br />
holding the channel metadata and list of connected consumers will require a<br />
watchdog cleaning up all the stale consumer connections. Second, as consumers<br />
join in and disappear, the channel server would need to maintain a consistent<br />
view of the list of consumers by the means of locks, with the incurred<br />
performance degradation of very large number of consumers. Third, as more<br />
consumers connect uniformly across the nodes, the more chattiness that will<br />
occur. At some point, all nodes will have consumer connections for a given<br />
channel. In order to to fulfill every operation, we must issue <code>N</code> requests to<br />
all nodes, where <code>N</code> is the number of nodes in the cluster. For the cluster to<br />
be able to process and deliver <code>M</code> messages, every node must be capable of<br />
processing <code>N*M</code> messages. This design will be limited in the number of<br />
connections it can hold, because of the centralized channel-consumer tracking<br />
problem, and will also only scale to the maximum request processing capacity<br />
of an individual node.</p>
<p>We can solve some of the operability challenges by removing the channel<br />
management of consumer connections, and instead of keeping a list, keeping the<br />
visibility of the peer nodes. Here the thinking is that since we will<br />
asymptotically reach the point where all nodes hold consumer connections for a<br />
given channel, all we really need to do is keep a list of all nodes in the<br />
cluster. Some centralized agent keeps a directory of all active peers holding<br />
consumer connections, e.g. a Zookeeper ensemble.</p>
<p><img src="{{ site.base_url }}/assets/design.png" alt="Ring-based Cluster" /></p>
<p>Consumers get uniformly connected to the nodes in the cluster by a "good"<br />
load-balancing scheme. Since any node can hold connections to consumers on any<br />
channel, there is therefore no snapshot of a channel's consumers, and to be<br />
able to identify all consumers connected to a channel it is necessary to<br />
interrogate all nodes in the cluster. Whereas this design improves the<br />
previous ones in that it allows scaling to an infinite number of connections,<br />
it will still only scale to the message processing throughput of an individual<br />
node.</p>
<p>A popular alternative to the directory of nodes is the tree of nodes. In this<br />
model, we start with a single node. As we reach the maximum number of<br />
connections the node can hold, we add two new nodes. The original node still<br />
accepts messages from publishers, but brokers the delivery to the two new<br />
nodes. As those nodes themselves become saturated, we add a new layer of four<br />
nodes. And so forth. This approach has the same limitation as the one using a<br />
directory of nodes, i.e. the maximum throughput is bound to that of the<br />
individual node.</p>
<p>We've seen how to hold the connections to an infinite number of consumers, but<br />
not how to deliver an infinite number of messages. These solutions scale very<br />
well to tens of thousands of messages and millions of active connected<br />
consumers, but have an upper limit. For most producers out there, that upper<br />
limit is probably high enough to be fine. But such limit exists in push-based<br />
systems.</p>
<p>Both the messaging literature and the messaging praxis have historically<br />
preferred using pull-based models rather than push-based ones. In a pull<br />
model, consumers come back to the broker to fetch messages, at each consumer's<br />
own rate, and the problem is therefore no longer dispatching across millions<br />
of connections. Pull-based messaging systems chose to store the messages until<br />
consumers come back to fetch them. In fact, the only scalable messaging system<br />
to millions of messages and millions of consumers that we know of uses store-<br />
and-forward: SMTP.</p>
<p>As much as I may think that the techniques that enable web push-models such as<br />
HTTP streaming, long-poll and WebSockets are genuinely useful to solve point<br />
problems, they are not techniques we can use to implement Internet-scale push-<br />
based web services, as they are fundamentally based on a PubSub model. The<br />
scalability of PubSub under high load remains an unresolved research question<br />
and as such is not a paradigm we should apply at Internet-scale.</p>
<p>In fact, I am now <em>almost</em> convinced that we've been looking at this in the<br />
wrong way, and that the right solution to this problem is a store-and-forward<br />
solution, where web consumers connect at their own rate to fetch messages and<br />
intermediaries throttle concurrent connection rates in order to achieve linear<br />
scalability. Essentially, this is a web of <em>partially connected store-and-<br />
forward almost real-time async data peers</em>. And that's a mouthful, but a really<br />
exciting one.</p>
